{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#AnimateDiffPipeline"
      ],
      "metadata": {
        "id": "4yl7IryNZ1Iw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgIIllJC_w04"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate diffusers imageio-ffmpeg\n",
        "import torch\n",
        "from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\n",
        "from diffusers.utils import export_to_gif\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "device = \"cuda\"\n",
        "dtype = torch.float16\n",
        "\n",
        "step = 4  # Options: [1,2,4,8]\n",
        "repo = \"ByteDance/AnimateDiff-Lightning\"\n",
        "ckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\n",
        "base = \"emilianJR/epiCRealism\"  # Choose to base model.\n",
        "\n",
        "adapter = MotionAdapter().to(device, dtype)\n",
        "adapter.load_state_dict(load_file(hf_hub_download(repo ,ckpt), device=device))\n",
        "pipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\n",
        "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")\n",
        "\n",
        "output = pipe(prompt=\"crack two eggs into a bowl\", guidance_scale=1.0, num_inference_steps=step)\n",
        "export_to_gif(output.frames[0], \"animation.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tuning\n"
      ],
      "metadata": {
        "id": "Z-TX9bhXe6t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLY16IFv1CZx",
        "outputId": "392aa4cd-8cec-44d9-808b-28373d29b8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from diffusers import AnimateDiffPipeline, MotionAdapter\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# Set up the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16"
      ],
      "metadata": {
        "id": "Lh5NumLbup-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Data path\n",
        "frame_dir = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/processed_data/train\"\n",
        "annotation_path = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/youcook/annotations/youcookii_annotations_trainval.json\"\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "class YouCook2Dataset(Dataset):\n",
        "    def __init__(self, annotation_path, frame_path, transform=None):\n",
        "        with open(annotation_path, \"r\") as f:\n",
        "            self.annotations = json.load(f)\n",
        "        self.frame_path = frame_path\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        # Get all video IDs in `frame_dir`\n",
        "        self.existing_video_ids = set([f.split(\"_sentence\")[0] for f in os.listdir(self.frame_path) if \"_frames\" in f])\n",
        "\n",
        "        # Load data\n",
        "        self.data = self.prepare_data()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\" Parse JSON to get video frames and corresponding captions \"\"\"\n",
        "        data = []\n",
        "\n",
        "        for video_id, info in self.annotations['database'].items():\n",
        "            if video_id not in self.existing_video_ids:\n",
        "                continue\n",
        "\n",
        "            for annotation_idx, annotation in enumerate(info['annotations']):\n",
        "                sentence = annotation[\"sentence\"]\n",
        "                sentence_folder = f\"{video_id}_sentence{annotation_idx}_frames\"\n",
        "                video_path = os.path.join(self.frame_path, sentence_folder)\n",
        "\n",
        "                if not os.path.exists(video_path):\n",
        "                    continue\n",
        "\n",
        "                # Get `frame_*.jpg` files\n",
        "                frames = sorted(glob.glob(os.path.join(video_path, \"frame_*.jpg\")))\n",
        "                if not frames:\n",
        "                    continue\n",
        "\n",
        "                data.append((frames[:8], sentence))  # Select the first 8 frames\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_paths, sentence = self.data[idx]\n",
        "        frames = torch.stack([self.transform(Image.open(fp).convert(\"RGB\")) for fp in frame_paths])\n",
        "        return frames, sentence\n",
        "\n",
        "dataset = YouCook2Dataset(annotation_path, frame_dir)\n",
        "print(\"Dataset loaded successfully, total samples:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJMwjfk9oRTk",
        "outputId": "44213645-84f1-4301-fed5-9a8f77fbc087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully, total samples: 8634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade transformers accelerate diffusers imageio-ffmpeg\n",
        "import torch\n",
        "import gc\n",
        "import numpy as np\n",
        "from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\n",
        "from diffusers.utils import export_to_gif\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "device = \"cuda\"\n",
        "dtype = torch.float16\n",
        "\n",
        "step = 4  # Options: [1,2,4,8]\n",
        "repo = \"ByteDance/AnimateDiff-Lightning\"\n",
        "ckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\n",
        "base = \"emilianJR/epiCRealism\"  # Choose the base model.\n",
        "\n",
        "adapter = MotionAdapter().to(device, dtype)\n",
        "adapter.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=device))\n",
        "pipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\n",
        "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")\n",
        "\n",
        "\n",
        "# Load pre-trained MotionAdapter\n",
        "repo = \"ByteDance/AnimateDiff-Lightning\"\n",
        "ckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\n",
        "\n",
        "adapter = MotionAdapter().to(device, dtype)\n",
        "adapter.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=device))  # Load the official MotionAdapter pre-trained model\n",
        "adapter.train()  # Set to training mode\n",
        "\n",
        "# Enable training for the entire MotionAdapter\n",
        "for param in adapter.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 3\n",
        "learning_rate = 3e-5\n",
        "# max_batches = 4  # Train more batches\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "optimizer = AdamW(adapter.parameters(), lr=learning_rate)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs // 2, eta_min=1e-6)  # Smooth learning rate decay\n",
        "\n",
        "# Attach the fine-tuned MotionAdapter\n",
        "pipe.motion_adapter = adapter\n",
        "\n",
        "# Train MotionAdapter (without LoRA)\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "        # if batch_idx >= max_batches:\n",
        "        #     break  # Train only the first 50 batches\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Load input data\n",
        "        frames, captions = batch\n",
        "        frames = frames.to(device, dtype)  # [batch, frames, channels, height, width]\n",
        "\n",
        "        # Process text input\n",
        "        if isinstance(captions, tuple):\n",
        "            prompts = [str(c) for c in captions]\n",
        "        elif isinstance(captions, list):\n",
        "            prompts = [str(c) for c in captions]\n",
        "        else:\n",
        "            prompts = [str(captions)]\n",
        "        prompt = \" \".join(prompts)\n",
        "\n",
        "        # Generate animation\n",
        "        output = pipe(prompt=prompt, guidance_scale=1.0, num_inference_steps=step)\n",
        "\n",
        "        # Ensure output.frames is a NumPy array\n",
        "        frames_np = np.array(output.frames)\n",
        "        print(f\"Shape of output.frames: {frames_np.shape}\")  # Debugging\n",
        "\n",
        "        # Ensure output.frames shape is (frames, H, W, C)\n",
        "        if frames_np.ndim == 5:  # Possibly (batch, frames, H, W, C)\n",
        "            frames_np = frames_np.squeeze(0)  # Remove batch dimension\n",
        "\n",
        "        # Convert to PyTorch Tensor, ensure requires_grad=True\n",
        "        generated_frames = torch.as_tensor(frames_np, device=device, dtype=dtype).clone().detach().requires_grad_() / 255.0\n",
        "\n",
        "        # Adjust channel order (frames, H, W, C) -> (frames, C, H, W)\n",
        "        generated_frames = generated_frames.permute(0, 3, 1, 2)\n",
        "\n",
        "        # Add batch dimension (batch, frames, C, H, W)\n",
        "        generated_frames = generated_frames.unsqueeze(0)\n",
        "\n",
        "        # Interpolate only supports (N, C, H, W), so flatten frames dimension first\n",
        "        b, f, c, h, w = generated_frames.shape\n",
        "        generated_frames = generated_frames.view(b * f, c, h, w)\n",
        "\n",
        "        # Resize to 256x256\n",
        "        generated_frames = torch.nn.functional.interpolate(\n",
        "            generated_frames, size=(256, 256), mode='bilinear', align_corners=False\n",
        "        )\n",
        "\n",
        "        # Restore frames dimension (batch, frames, C, H, W)\n",
        "        generated_frames = generated_frames.view(b, f, c, 256, 256)\n",
        "\n",
        "        # Ensure consistent number of frames\n",
        "        min_frames = min(generated_frames.shape[1], frames.shape[1])\n",
        "        generated_frames = generated_frames[:, :min_frames, :, :, :]\n",
        "        frames = frames[:, :min_frames, :, :, :]\n",
        "\n",
        "        # Ensure frames also require gradients\n",
        "        frames = frames.clone().detach().requires_grad_()\n",
        "\n",
        "        # Compute loss\n",
        "        # loss = F.mse_loss(generated_frames, frames)\n",
        "        loss = F.smooth_l1_loss(generated_frames, frames, beta=0.5)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Save the fully fine-tuned MotionAdapter\n",
        "torch.save(adapter.state_dict(), \"/content/drive/Shareddrives/DATA 298B Team 8/CODE/motion_adapter_finetuned_4.6.pth\")\n",
        "print(\"Training complete, saved motion_adapter_finetuned.pth\")\n",
        "\n",
        "# Reload the fully fine-tuned MotionAdapter\n",
        "adapter = MotionAdapter().to(device, dtype)\n",
        "\n",
        "# Directly load the full fine-tuned MotionAdapter weights\n",
        "state_dict = torch.load(\"/content/drive/Shareddrives/DATA 298B Team 8/CODE/motion_adapter_finetuned_4.6.pth\", map_location=device)  # No need for `weights_only=True`\n",
        "adapter.load_state_dict(state_dict, strict=True)  # `strict=True` ensures weights match exactly\n",
        "\n",
        "pipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\n",
        "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")\n",
        "\n",
        "output = pipe(prompt=\"crack two eggs into a bowl\", guidance_scale=1.0, num_inference_steps=step)\n",
        "export_to_gif(output.frames[0], \"/content/drive/Shareddrives/DATA 298B Team 8/CODE/AnimateDiff_finetuned_animation_4.6.gif\")\n"
      ],
      "metadata": {
        "id": "tRdeJ1HwcqTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio diffusers huggingface_hub safetensors torch torchvision\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "from PIL import Image\n",
        "\n",
        "# Define Gradio processing function\n",
        "def generate_animation(prompt):\n",
        "    \"\"\"Generate an animation based on the user input text.\"\"\"\n",
        "    output = pipe(prompt=prompt, guidance_scale=1.0, num_inference_steps=step)\n",
        "\n",
        "    # Ensure output is in NumPy format\n",
        "    frames_np = np.array(output.frames, dtype=np.float32)  # Ensure float32 for precision\n",
        "\n",
        "    # Handle dimensions\n",
        "    if frames_np.ndim == 5:  # Possibly (batch, frames, H, W, C)\n",
        "        frames_np = frames_np.squeeze(0)\n",
        "\n",
        "    # Normalize values to range [0, 1] if they are outside the range\n",
        "    if frames_np.min() < 0 or frames_np.max() > 1:\n",
        "        frames_np = (frames_np - frames_np.min()) / (frames_np.max() - frames_np.min())\n",
        "\n",
        "    # Convert to RGB format (if necessary, some models output BGR or grayscale)\n",
        "    if frames_np.shape[-1] == 3:  # Assuming (frames, H, W, C)\n",
        "        frames_np = frames_np[..., ::-1]  # Convert BGR to RGB if necessary\n",
        "\n",
        "    # Convert NumPy array to PIL format\n",
        "    frames_pil = [Image.fromarray((frame * 255).astype(np.uint8), mode=\"RGB\") for frame in frames_np]\n",
        "\n",
        "    # Generate GIF\n",
        "    gif_path = \"/content/generated_animation.gif\"\n",
        "    frames_pil[0].save(gif_path, save_all=True, append_images=frames_pil[1:], duration=100, loop=0)\n",
        "\n",
        "    return gif_path\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=generate_animation,\n",
        "    inputs=gr.Textbox(label=\"Enter a text prompt\"),\n",
        "    outputs=gr.Image(type=\"filepath\", label=\"Generated Animation\"),\n",
        "    title=\"Text to Video Web App\",\n",
        "    description=\"Enter a text description and generate an animation.\"\n",
        ")\n",
        "\n",
        "# Launch Web App\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-bHjzFx-4P6o",
        "outputId": "9b46280a-89f8-4e60-fa9f-ac2e582cf669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.20.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.11)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://08008f3358ae9b0d96.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://08008f3358ae9b0d96.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXd5FxNkV9ue",
        "outputId": "997207d1-3f7e-4f4e-c6c7-7ab31c8c9042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from lpips) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from lpips) (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.11/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.11/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.2.1->lpips) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lpips\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lpips-0.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute all evaluation metrics for each comparison"
      ],
      "metadata": {
        "id": "FiGcDHrV_S_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install piq\n",
        "\n",
        "import os, json, glob, random, gc\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import imageio\n",
        "import numpy as np\n",
        "import lpips\n",
        "import piq\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Load annotation & sample ===\n",
        "annotation_path = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/youcook/annotations/youcookii_annotations_trainval.json\"\n",
        "frame_dir = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/processed_data/val\"\n",
        "\n",
        "with open(annotation_path, \"r\") as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "resize_only = transforms.Resize((256, 256))\n",
        "\n",
        "existing_ids = set([f.split(\"_sentence\")[0] for f in os.listdir(frame_dir) if \"_frames\" in f])\n",
        "valid_samples = []\n",
        "\n",
        "for vid, info in annotations[\"database\"].items():\n",
        "    if vid not in existing_ids: continue\n",
        "    for i, ann in enumerate(info[\"annotations\"]):\n",
        "        folder = f\"{vid}_sentence{i}_frames\"\n",
        "        path = os.path.join(frame_dir, folder)\n",
        "        frames = sorted(glob.glob(os.path.join(path, \"frame_*.jpg\")))\n",
        "        if os.path.isdir(path) and len(frames) >= 1:\n",
        "            valid_samples.append((frames[:1], ann[\"sentence\"]))\n",
        "\n",
        "frame_paths, prompt = random.choice(valid_samples)\n",
        "print(\"Prompt:\", prompt)\n",
        "\n",
        "# === Load 1 frame from GIF\n",
        "gif_path = \"/content/drive/Shareddrives/DATA 298B Team 8/CODE/AnimateDiff_finetuned_animation_4.6.gif\"\n",
        "gif = imageio.mimread(gif_path)\n",
        "gen_img = Image.fromarray(gif[0])  # only the first frame\n",
        "\n",
        "# === LPIPS + SSIM + PSNR\n",
        "loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "real_tensor = transform(Image.open(frame_paths[0]).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "gen_tensor = transform(gen_img).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    lpips_score = loss_fn(real_tensor, gen_tensor).item()\n",
        "    ssim_score = piq.ssim(gen_tensor, real_tensor, data_range=1.0).item()\n",
        "    psnr_score = piq.psnr(gen_tensor, real_tensor, data_range=1.0).item()\n",
        "\n",
        "del real_tensor, gen_tensor\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# === FVD (over single frame)\n",
        "def fvd_single(real_path, gen_pil):\n",
        "    r = np.array(resize_only(Image.open(real_path))).astype(np.float32).flatten()\n",
        "    g = np.array(resize_only(gen_pil)).astype(np.float32).flatten()\n",
        "    mu1, mu2 = r.mean(), g.mean()\n",
        "    sigma1, sigma2 = r.std(), g.std()\n",
        "    return (mu1 - mu2) ** 2 + (sigma1 + sigma2 - 2 * np.sqrt(sigma1 * sigma2))\n",
        "\n",
        "fvd_score = fvd_single(frame_paths[0], gen_img)\n",
        "\n",
        "# === CLIP (1 frame only)\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "clip_inputs = clip_processor(text=prompt, images=gen_img, return_tensors=\"pt\").to(device)\n",
        "clip_outputs = clip_model(**clip_inputs)\n",
        "clip_score = clip_outputs.logits_per_image.mean().item()\n",
        "\n",
        "# === Final output\n",
        "print({\n",
        "    \"Prompt\": prompt,\n",
        "    \"LPIPS\": lpips_score,\n",
        "    \"SSIM\": ssim_score,\n",
        "    \"PSNR\": psnr_score,\n",
        "    \"FVD\": fvd_score,\n",
        "    \"CLIP Score\": clip_score\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585,
          "referenced_widgets": [
            "126ecf69833d49ccb4049d09b6a11122",
            "63abbcd5839a4323a8d341c4e5a8743a",
            "906799c147234032a0b73f2dec4fbc2f",
            "08e4f70f58e34dc3bd5d1907c1f801d1",
            "846d6c55110946d2a1abc8661c67ec70",
            "955d267c7253447dae9192edecfa03de",
            "3497af21d3364f1e81c88482adc5caf9",
            "194c677888244058a9d5b228f1f90ace",
            "eb113b0afed947acac00f6fb4c23bbb7",
            "98fcd8350bbd46d480a07e3501cae0e6",
            "5e645a49799547a2a3e901d396c8cc23",
            "8a729a80a09648c3b372dd8208570a1a",
            "c2b6c7967d924a72b185efa9ea64ae86",
            "722f765794fd46de802ba8e08287c2d6",
            "b71066830b064d71afe72e8447a8265b",
            "631e7a8de5734276bc9ca6e245888c74",
            "d812a401ce9b407297cd71eddd1b5a1f",
            "3a53c0501d36457cb6f4ba55cf336e28",
            "d92a85d601904563a264a9b2e00752ca",
            "1058d4b8964f4d0995de0542268623d2",
            "a27f3482c8d848c88ca16354b7a69ca2",
            "4c095f90364341aa8d48edd660ae3f30",
            "0f02aedf4a844992a79579d411dcbe89",
            "a79f3224abf74a6081d39147aca2fff0",
            "67e90f72728145b4ad329eacb1d30022",
            "3993bbc9fd9a4f61ab44bfe90d2da428",
            "de57b967a0a7486f9261ed00ff608407",
            "4ef81828ca8d4d51a57e484b549633d4",
            "45b61e7859124c7eb1672f83cd973df0",
            "7f0351e971e54a459024abf0b9378e32",
            "44ba06ba1ee34c1c826a0437154c5868",
            "85375c059bb54b09b07f606b8a09bbec",
            "026fecf0183443b5874799c087c4ff56",
            "1b862ca05ce042fdae0a3cde7a4e3099",
            "4dc7a6bf78a843ff8e5b7d3868cbbd56",
            "e33a317dc3384a6986a1e4666906d23e",
            "fe4f9ec347074e5d8370303b538b8096",
            "be1f1fa4c1f9438296b3d10978cdafc9",
            "d599b5e255af4869a3cc38afd6314455",
            "0ba6ad493eba4c75814a1e7cd421a0ea",
            "45862c5808604485bc11628cac260f0f",
            "5c1f71d1bba946a0983746155740b3f7",
            "5a94dc9d41834055ab53836be2952b8f",
            "4a45571edc91473aabdddcec28cce858",
            "c5a5b00cdfa949449e97c4e2b5adcada",
            "8dabcf78d6f747d6af4f96914e8b7957",
            "3a6ae35ed3a64a2f8b06b7b9773a4c66",
            "47460cfaafa44b0388487d7cfd292687",
            "a9a40318addc4441bce62613b2bd3a8a",
            "3902f1c4a8c1401c8f8dfe3b2d975cc1",
            "bc0daaea7a634badbe5e7016204ee445",
            "d70dcdf9053141aab02427c5ba201a7f",
            "5f07b309e7e543658ce0bfb63f83a8ce",
            "3f6497a72bbb4817b37594234a9df320",
            "ba0d70f1cf634932a82fa8f0aa38d22b",
            "9eb8334ca1bd4117b3618b37946e1f88",
            "337bc1e0600f46db991459583fcb862a",
            "3e8b1fcbe5174f6b876bd3e4e382ce21",
            "51b81c5b89a74deca87d2c6a2b13b47a",
            "217ed4951af54f1da23c45aa3fbdab94",
            "12ba889583494ba0adf1453d274fbd03",
            "fe1be7369bba40faaf8c043064b899c6",
            "c067de72c0ed4fa8ac49396b1bc2d2f3",
            "30b140bddaf241c1888605ef673cc6af",
            "2d72ef8e4dd747f9a123b8cc0a1abd3b",
            "90b1e4148cf144b98204f3c073bde39c",
            "83e1cde4c12446d49073c5e5d55e2782",
            "954e4fee13f94ca4a33816dc0f57a974",
            "e7aed1b695334832ad7ce0df34ed2328",
            "4c47a7309ab448fab7d6e3263eaa433f",
            "e43b089bb6e84c20aae758b216edf645",
            "7d88fef5baa24a92b647efe1e39f75c2",
            "f7209576a2c247c4b20164ab84d99b9c",
            "be082827b9ca40a1b308805a6337d00e",
            "895b6597d36d44f5a694337f128ee733",
            "c6ded3f7deee4fa482b0b17d9a0f6c2c",
            "162ea9f712544ec7b6365187490492a2",
            "8299dff8ded0443e8cdec96131fdbed1",
            "43820580675e4bfa8132d094ffac403f",
            "8dfca53091ec49f3a15a8282ab823d6f",
            "f036d3ac57f94d48a735bb6a32273120",
            "5276c397fa2f4e44a7bb9145c92966f0",
            "90cc6a860d184d0db7bcb1d2792a87f4",
            "df62d3d57c504b5bbc15e9074228367f",
            "5dd228fa8f8e42669df6578f104eac04",
            "80b3eb2113174058be73ebc7853b3840",
            "63753e19af584661b6c670b8fa26e67d",
            "2a23e213116547169b96d29844e57f3f",
            "16a9bd7e7a59427f8452b785c3e63fa6",
            "918fa7afb2d24ee59d9f7aadb83bad1c",
            "aa7302fcdf8640c69ae5d4d7d4a470c4",
            "ec879c86978b4c958ddb859a8b75d5dd",
            "bf0769701afe4643b8607721f8de37bf",
            "75fc62cf2031443ea7df7a502f22c76c",
            "a6108ce9f09e4e68b13898bec25382f8",
            "d8b32e331c254c08b194d6e5657e43ad",
            "a8c5fb6d736b4b0c9d4ac988c08b543b",
            "c82f09276d29409ca4e25f9fd09e6cae",
            "b724074cf7804148a35910a87b6f161a"
          ]
        },
        "outputId": "5f9d67db-23bf-4099-b108-a124755db5f6",
        "id": "_PJOmktdKaE1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: use a tablespoon to make balls out of the mashed potatoes\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "126ecf69833d49ccb4049d09b6a11122"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a729a80a09648c3b372dd8208570a1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f02aedf4a844992a79579d411dcbe89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b862ca05ce042fdae0a3cde7a4e3099"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5a5b00cdfa949449e97c4e2b5adcada"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eb8334ca1bd4117b3618b37946e1f88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83e1cde4c12446d49073c5e5d55e2782"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8299dff8ded0443e8cdec96131fdbed1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16a9bd7e7a59427f8452b785c3e63fa6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Prompt': 'use a tablespoon to make balls out of the mashed potatoes', 'LPIPS': 0.7376506328582764, 'SSIM': 0.0040740687400102615, 'PSNR': 5.727221488952637, 'FVD': np.float32(13616.206), 'CLIP Score': 26.30241584777832}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import imageio\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Grayscale\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "annotation_path = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/youcook/annotations/youcookii_annotations_trainval.json\"\n",
        "frame_dir = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/processed_data/val\"\n",
        "gif_path = \"/content/drive/Shareddrives/DATA 298B Team 8/CODE/AnimateDiff_finetuned_animation_4.6.gif\"\n",
        "resize_only = transforms.Resize((256, 256))\n",
        "# ============================\n",
        "\n",
        "# === Load annotations\n",
        "with open(annotation_path, \"r\") as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "existing_ids = set([f.split(\"_sentence\")[0] for f in os.listdir(frame_dir) if \"_frames\" in f])\n",
        "\n",
        "# === Find valid sample\n",
        "valid_samples = []\n",
        "for vid, info in annotations[\"database\"].items():\n",
        "    if vid not in existing_ids:\n",
        "        continue\n",
        "    for i, ann in enumerate(info[\"annotations\"]):\n",
        "        folder = f\"{vid}_sentence{i}_frames\"\n",
        "        path = os.path.join(frame_dir, folder)\n",
        "        frames = sorted(glob.glob(os.path.join(path, \"frame_*.jpg\")))\n",
        "        if os.path.isdir(path) and len(frames) >= 2:\n",
        "            valid_samples.append((frames[:2], ann[\"sentence\"]))\n",
        "\n",
        "# === Pick one sample\n",
        "frame_paths, prompt = valid_samples[0]\n",
        "gif = imageio.mimread(gif_path)\n",
        "\n",
        "resize_64_gray = Compose([\n",
        "    Resize((64, 64)),\n",
        "    Grayscale(),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "def fvd_lightweight(real_paths, gen_frames, num_frames=2):\n",
        "    real_feats, gen_feats = [], []\n",
        "    for i in range(num_frames):\n",
        "        r = resize_64_gray(Image.open(real_paths[i])).view(-1).numpy()\n",
        "        g = resize_64_gray(Image.fromarray(gen_frames[i])).view(-1).numpy()\n",
        "        real_feats.append(r)\n",
        "        gen_feats.append(g)\n",
        "    real_feats = np.stack(real_feats)\n",
        "    gen_feats = np.stack(gen_feats)\n",
        "\n",
        "    mu_r, mu_g = real_feats.mean(0), gen_feats.mean(0)\n",
        "    sigma_r = np.cov(real_feats, rowvar=False)\n",
        "    sigma_g = np.cov(gen_feats, rowvar=False)\n",
        "\n",
        "    diff = np.sum((mu_r - mu_g) ** 2)\n",
        "    trace_cov = np.trace(sigma_r) + np.trace(sigma_g) - 2 * np.sqrt(np.trace(sigma_r) * np.trace(sigma_g))\n",
        "    return float(diff + trace_cov)\n",
        "\n",
        "# === Compute FVD\n",
        "n_fvd = min(2, len(frame_paths), len(gif))\n",
        "fvd_score = fvd_lightweight(frame_paths, gif, num_frames=n_fvd)\n",
        "\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"FVD (2-frame): {fvd_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I97NqV6eHFMt",
        "outputId": "be774064-8e53-4687-a295-86631a2957f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: heat some oil in a pan with garlic and cumin seeds\n",
            "FVD (2-frame): 436.5696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install piq lpips transformers imageio\n",
        "\n",
        "import os, json, glob, random, gc\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import imageio\n",
        "import numpy as np\n",
        "import piq\n",
        "import lpips\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torch.nn.functional import normalize\n",
        "import pandas as pd\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "annotation_path = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/youcook/annotations/youcookii_annotations_trainval.json\"\n",
        "frame_dir = \"/content/drive/Shareddrives/DATA 298B Team 8/298A/processed_data/val\"\n",
        "gif_path = \"/content/drive/Shareddrives/DATA 298B Team 8/CODE/AnimateDiff_finetuned_animation_4.6.gif\"\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "resize_only = transforms.Resize((256, 256))\n",
        "resize_64_gray = transforms.Compose([transforms.Resize((64, 64)), transforms.Grayscale(), transforms.ToTensor()])\n",
        "\n",
        "# === Load annotation\n",
        "with open(annotation_path, \"r\") as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "existing_ids = set([f.split(\"_sentence\")[0] for f in os.listdir(frame_dir) if \"_frames\" in f])\n",
        "\n",
        "valid_samples = []\n",
        "for vid, info in annotations[\"database\"].items():\n",
        "    if vid not in existing_ids:\n",
        "        continue\n",
        "    for i, ann in enumerate(info[\"annotations\"]):\n",
        "        folder = f\"{vid}_sentence{i}_frames\"\n",
        "        path = os.path.join(frame_dir, folder)\n",
        "        frames = sorted(glob.glob(os.path.join(path, \"frame_*.jpg\")))\n",
        "        if os.path.isdir(path) and len(frames) >= 2:\n",
        "            valid_samples.append((frames, ann[\"sentence\"]))\n",
        "\n",
        "print(f\"Total valid samples: {len(valid_samples)}\")\n",
        "\n",
        "# === Load shared GIF and models\n",
        "gif_frames = imageio.mimread(gif_path)\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
        "clip_model.eval()\n",
        "\n",
        "def fvd_lightweight(real_paths, gen_frames, num_frames=2):\n",
        "    real_feats, gen_feats = [], []\n",
        "    for i in range(num_frames):\n",
        "        r = resize_64_gray(Image.open(real_paths[i])).view(-1).numpy()\n",
        "        g = resize_64_gray(Image.fromarray(gen_frames[i])).view(-1).numpy()\n",
        "        real_feats.append(r)\n",
        "        gen_feats.append(g)\n",
        "    real_feats = np.stack(real_feats)\n",
        "    gen_feats = np.stack(gen_feats)\n",
        "    mu_r, mu_g = real_feats.mean(0), gen_feats.mean(0)\n",
        "    sigma_r = np.cov(real_feats, rowvar=False)\n",
        "    sigma_g = np.cov(gen_feats, rowvar=False)\n",
        "    diff = np.sum((mu_r - mu_g) ** 2)\n",
        "    trace_cov = np.trace(sigma_r) + np.trace(sigma_g) - 2 * np.sqrt(np.trace(sigma_r) * np.trace(sigma_g))\n",
        "    return float(diff + trace_cov)\n",
        "\n",
        "def compute_clip_score(prompt, gen_pil):\n",
        "    inputs = clip_processor(text=prompt, images=gen_pil, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(inputs[\"pixel_values\"])\n",
        "        text_features = clip_model.get_text_features(inputs[\"input_ids\"])\n",
        "        image_features = normalize(image_features, dim=-1)\n",
        "        text_features = normalize(text_features, dim=-1)\n",
        "        score = (image_features * text_features).sum(dim=-1).mean().item()\n",
        "    return score\n",
        "\n",
        "# === Main Loop\n",
        "results = []\n",
        "samples = random.sample(valid_samples, k=10)\n",
        "\n",
        "for idx, (frame_paths, prompt) in enumerate(samples):\n",
        "    real_img = transform(Image.open(frame_paths[0]).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    gen_img = transform(Image.fromarray(gif_frames[0])).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        lpips_score = lpips_fn(real_img, gen_img).item()\n",
        "        ssim_score = piq.ssim(gen_img, real_img, data_range=1.0).item()\n",
        "        psnr_score = piq.psnr(gen_img, real_img, data_range=1.0).item()\n",
        "\n",
        "    # FVD\n",
        "    fvd_score = fvd_lightweight(frame_paths, gif_frames, num_frames=2)\n",
        "\n",
        "    # CLIP\n",
        "    clip_score = compute_clip_score(prompt, Image.fromarray(gif_frames[0]))\n",
        "\n",
        "    results.append({\n",
        "        \"Prompt\": prompt,\n",
        "        \"LPIPS\": round(lpips_score, 4),\n",
        "        \"SSIM\": round(ssim_score, 4),\n",
        "        \"PSNR\": round(psnr_score, 4),\n",
        "        \"FVD (2f)\": round(fvd_score, 4),\n",
        "        \"CLIP Score\": round(clip_score, 4)\n",
        "    })\n",
        "\n",
        "    print(f\"[{idx+1}/10] Done | LPIPS: {lpips_score:.4f}, SSIM: {ssim_score:.4f}, \"\n",
        "          f\"PSNR: {psnr_score:.2f}, FVD: {fvd_score:.2f}, CLIP: {clip_score:.4f}\")\n",
        "\n",
        "# === Results table\n",
        "df = pd.DataFrame(results)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB22uxmWMiee",
        "outputId": "923e3447-ad8b-4d8c-d09b-ff27376460f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total valid samples: 1813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "[1/10] Done | LPIPS: 0.7640, SSIM: 0.0004, PSNR: 5.69, FVD: 1112.74, CLIP: 0.2042\n",
            "[2/10] Done | LPIPS: 0.7044, SSIM: 0.2095, PSNR: 10.73, FVD: 457.70, CLIP: 0.1879\n",
            "[3/10] Done | LPIPS: 0.7092, SSIM: 0.0031, PSNR: 5.76, FVD: 906.55, CLIP: 0.1812\n",
            "[4/10] Done | LPIPS: 0.6062, SSIM: 0.2181, PSNR: 8.44, FVD: 596.12, CLIP: 0.1741\n",
            "[5/10] Done | LPIPS: 0.7640, SSIM: 0.0004, PSNR: 5.69, FVD: 963.22, CLIP: 0.2537\n",
            "[6/10] Done | LPIPS: 0.7626, SSIM: 0.0111, PSNR: 5.75, FVD: 1104.03, CLIP: 0.1982\n",
            "[7/10] Done | LPIPS: 0.7640, SSIM: 0.0004, PSNR: 5.69, FVD: 939.83, CLIP: 0.2216\n",
            "[8/10] Done | LPIPS: 0.7220, SSIM: 0.3831, PSNR: 11.27, FVD: 357.69, CLIP: 0.2287\n",
            "[9/10] Done | LPIPS: 0.6656, SSIM: 0.1122, PSNR: 8.21, FVD: 548.61, CLIP: 0.2149\n",
            "[10/10] Done | LPIPS: 0.6479, SSIM: 0.1428, PSNR: 9.30, FVD: 524.67, CLIP: 0.2193\n",
            "                                              Prompt   LPIPS    SSIM     PSNR  \\\n",
            "0                     grate onion and add to potatos  0.7640  0.0004   5.6878   \n",
            "1                         slice the onion and carrot  0.7044  0.2095  10.7287   \n",
            "2                              cut the dark part off  0.7092  0.0031   5.7574   \n",
            "3  add paneer cubes to a pan with hot oil and sau...  0.6062  0.2181   8.4354   \n",
            "4  form the meat into balls place them on a cooki...  0.7640  0.0004   5.6878   \n",
            "5  add the bean sprouts and the garlic chives and...  0.7626  0.0111   5.7503   \n",
            "6                        transfer the meat to a bowl  0.7640  0.0004   5.6878   \n",
            "7                                    add lemon juice  0.7220  0.3831  11.2660   \n",
            "8                  add the chicken pieces to the pot  0.6656  0.1122   8.2119   \n",
            "9         add the spices and curry leaves to the pan  0.6479  0.1428   9.3022   \n",
            "\n",
            "    FVD (2f)  CLIP Score  \n",
            "0  1112.7442      0.2042  \n",
            "1   457.6988      0.1879  \n",
            "2   906.5519      0.1812  \n",
            "3   596.1167      0.1741  \n",
            "4   963.2179      0.2537  \n",
            "5  1104.0340      0.1982  \n",
            "6   939.8271      0.2216  \n",
            "7   357.6936      0.2287  \n",
            "8   548.6146      0.2149  \n",
            "9   524.6666      0.2193  \n"
          ]
        }
      ]
    }
  ]
}